{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. what do you want to acquire in this course？\n",
    "    Ans:  WeChat public number\n",
    "### 2. what problems do you want to solve？\n",
    "    Ans: Improve my skills\n",
    "### 3. what’s the advantages you have to finish you goal?\n",
    "    Ans: For it, everything is willing\n",
    "### 4. what’s the disadvantages you need to overcome to finish you goal?\n",
    "    Ans: I need to spend a lot of time learning it while I work.\n",
    "### 5. How will you plan to study in this course period?\n",
    "    Ans: First of all, I have to finish my homework on time and second, and then spend more time doing exercises after class.\n",
    "    \n",
    "    I have sent the answer to the specified mailbox for the above question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Can you come up out 3 sceneraies which use AI methods?\n",
    "    Ans: Face Recognition(人脸识别)、Driverless(无人驾驶)、Recommendation System(推荐系统) and so on\n",
    "### 1. How do we use Github; Why do we use Jupyter and Pycharm;\n",
    "    Ans: Github is a tool for managing code that makes it easy for multiple developers to work together.\n",
    "         Jupyter is usually used for simple development and testing, while Pycharm is used for real build and development projects.\n",
    "### 2. What's the Probability Model?\n",
    "    Ans: Probability Model is a model that uses the probability calculation of words\n",
    "### 3. Can you came up with some sceneraies at which we could use Probability Model?\n",
    "    Ans: Article classification、Intelligent question and answering system\n",
    "### 4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match?\n",
    "    Ans: Probability reflects the likelihood of a random event occurring. Because of the diversity of grammar and the correlation between words, we must not only consider the correlation between words before and after, but also consider the whole application scenario.\n",
    "### 5. What's the Language Model;\n",
    "    Ans: The language model is a model used to calculate the probability of a sentence appearing.\n",
    "### 6. Can you came up with some sceneraies at which we could use Language Model?\n",
    "    Ans: Lie detector(测谎仪)、Intelligent translation(智能翻译)\n",
    "### 7. What's the 1-gram language model;\n",
    "    Ans: Use the probability of each word to measure the probability of a sentence. \n",
    "### 8. What's the disadvantages and advantages of 1-gram language model;\n",
    "    Ans: The advantage is that the calculation is simple and easy to understand. The disadvantage is that the performance of the model is poor and the accuracy is low.\n",
    "### 9. What't the 2-gram models;\n",
    "    Ans: Use the probability of two words to measure the probability of a sentence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编程实践部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、设计你自己的句子生成器\n",
    "\n",
    "    请定义你自己的语法:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一个语法：\n",
    "boss = '''\n",
    "boss = 称呼 问句 结尾\n",
    "称呼 = 名字 | 称谓 名字\n",
    "称谓 = null | 你好， | 您好，\n",
    "名字 = 小孙 | 孙悟空 | 小高\n",
    "问句 = 询问 人称 动词 时间 事情\n",
    "询问 = 请问 | 问一下\n",
    "人称 = 你 | 你们 | 他 | 他们\n",
    "动词 = 知道 | 晓得 | 清楚\n",
    "时间 = 昨天 | 今天 | 明天 | 第 具体时间 天\n",
    "具体时间 = 单个数字 | 单个数字 单个数字\n",
    "单个数字 = 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9\n",
    "事情 = 要上班 | 要聚餐 | 要放假\n",
    "结尾 = 吗？ | 是吗？ | 对吗？\n",
    "'''\n",
    "# 第二个语法：\n",
    "gao = '''\n",
    "gao = 称呼 内容\n",
    "称呼 = 名字 ， 称谓\n",
    "称谓 = null | 你好！ | 您好！\n",
    "名字 = 老高 | 高老师 | 小高 | 高俅 | 高晓松\n",
    "内容 = 名字 的 事情 | 名字 动词 类别\n",
    "事情 = 课程 讲的 形容词\n",
    "课程 = nlp | cv\n",
    "形容词 = 太好了 | 太垃圾了 | 非常完美\n",
    "动词 = 喜欢 | 讨厌 | 热爱\n",
    "类别 = 美女 | 帅哥 | 动物\n",
    "动物 = 小狗 | 小猫 | 小动物\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数: grammer:string,line_split行分隔符,word_split词映射分割符,symbol_split符号分割符\n",
    "# 方法: 格式化语法模板, 返回dict(k=string,v=list(string))\n",
    "def create_grammer(grammer, line_split='\\n', word_split='=', symbol_split='|'):\n",
    "    grammer_map = {}\n",
    "    for line in grammer.split(line_split):\n",
    "        if line: \n",
    "            grammer_map[line.split(word_split)[0].strip()]=[[word.strip()] \\\n",
    "            for word in line.split(word_split)[1].strip().split(symbol_split)]\n",
    "    return grammer_map\n",
    "\n",
    "# 参数: grammer_map:dict, target:string\n",
    "# 方法: 根据语法模板创建句子, 返回string\n",
    "def create_centence(grammer_map, target='boss'):\n",
    "    sentence = ''\n",
    "    if target == 'null': return sentence\n",
    "    if target not in grammer_map: return target\n",
    "    line = random.choice(grammer_map[target])\n",
    "    for word in line[0].split(' '):\n",
    "        sentence += create_centence(grammer_map, target=word)\n",
    "    return sentence\n",
    "\n",
    "# 参数: grammer_map:dict, target:string, n:int表示生成句子数\n",
    "# 方法: 根据语法模板生成n条句子\n",
    "def generate_n(grammer_map, target='boss', n=10):\n",
    "    for _ in range(n): print(create_centence(grammer_map, target)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'孙悟空请问他知道第9天要上班是吗？'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第一个例子\n",
    "create_centence(create_grammer(boss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "孙悟空问一下你们知道今天要上班对吗？\n",
      "孙悟空问一下他们晓得第4天要放假吗？\n",
      "小高请问你们晓得明天要放假对吗？\n",
      "小孙请问他知道第4天要上班对吗？\n",
      "您好，小高请问他们清楚明天要上班是吗？\n",
      "小孙问一下他们知道第8天要放假对吗？\n",
      "小孙问一下他们清楚明天要放假吗？\n",
      "你好，孙悟空请问你清楚昨天要放假对吗？\n",
      "孙悟空请问你们知道第36天要上班是吗？\n",
      "孙悟空请问他们晓得昨天要放假是吗？\n"
     ]
    }
   ],
   "source": [
    "generate_n(create_grammer(boss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'高老师，小高的cv讲的太垃圾了'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第二个例子\n",
    "create_centence(create_grammer(gao), target='gao')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "老高，老高的nlp讲的非常完美\n",
      "老高，你好！小高热爱美女\n",
      "高晓松，您好！老高的nlp讲的太好了\n",
      "老高，你好！高晓松热爱小狗\n",
      "高老师，您好！高俅讨厌美女\n"
     ]
    }
   ],
   "source": [
    "generate_n(create_grammer(gao), target='gao', n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、使用新数据源完成语言模型的训练\n",
    "    \n",
    "    按照我们上文中定义的prob_2函数，我们更换一个文本数据源，获得新的Language Model:\n",
    "    下载文本数据集（你可以在以下数据集中任选一个，也可以两个都使用）\n",
    "    \n",
    "    可选数据集1，保险行业问询对话集： \n",
    "    https://github.com/Computing-Intelligence/insuranceqa-corpus-zh/raw/release/corpus/pool/train.txt.gz\n",
    "    \n",
    "    可选数据集2：豆瓣评论数据集：\n",
    "    https://github.com/Computing-Intelligence/datasource/raw/master/movie_comments.csv\n",
    "    \n",
    "    修改代码，获得新的2-gram语言模型\n",
    "    进行文本清洗，获得所有的纯文本\n",
    "    将这些文本进行切词\n",
    "    送入之前定义的语言模型中，判断文本的合理程度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建语言模型（language model）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数: s:string\n",
    "# 方法: 正则去掉特殊字符\n",
    "def delete_special_symbol(s):\n",
    "    return re.findall('\\w+', str(s))\n",
    " \n",
    "# 参数: s:string\n",
    "# 方法: 切词\n",
    "def cut(s):\n",
    "    return list(jieba.cut(s))\n",
    "\n",
    "# 参数: all_list:list(string)\n",
    "# 方法: 统计单个词\n",
    "def get_all_token(all_list):\n",
    "    token = []\n",
    "    for ele in all_list:\n",
    "        token += cut(ele)\n",
    "    return token\n",
    "\n",
    "# 参数: all_token:dict\n",
    "# 方法: 统计两个词拼接\n",
    "def get_all_token_two(all_token):\n",
    "    return [all_token[i]+all_token[i+1] for i in range(len(all_token)-1)]\n",
    "\n",
    "# 参数: word_1:string, word_2:string, all_token:dict\n",
    "# 方法: 计算前后两个词语联合出现的概率\n",
    "def get_prob_two(word_1,word_2, all_token):\n",
    "    token_two = get_all_token_two(all_token)\n",
    "    words_count_two = Counter(token_two)\n",
    "    if word_1+word_2 in words_count_two:\n",
    "        return words_count_two[word_1+word_2] / len(token_two)\n",
    "    else:\n",
    "        return 1 / len(token_two)\n",
    "    \n",
    "# 参数: sentence:string, token:dict\n",
    "# 方法: 某一个句子出现的概率\n",
    "def get_prob_sentence(sentence, token):\n",
    "    words = cut(sentence)\n",
    "    sentence_prob = 1\n",
    "    for i in range(len(words)-1):\n",
    "        next_word = words[i+1]\n",
    "        p2 = get_prob_two(words[i],next_word, token)\n",
    "        sentence_prob *= p2\n",
    "    return sentence_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1、保险行业问询对话集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入数据\n",
    "load_insurance_data = pd.read_table('/Users/yuanjin/PycharmProjects/nlp_data_set/datasource/train.txt',\n",
    "                                    sep='\\t',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0 ++$++ disability-insurance ++$++ 法律要求残疾保险吗？ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 ++$++ life-insurance ++$++ 债权人可以在死后人寿保险吗？ ++...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2 ++$++ renters-insurance ++$++ 旅行者保险有租赁保险吗？ +...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3 ++$++ auto-insurance ++$++ 我可以开一辆没有保险的新车吗？ +...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4 ++$++ life-insurance ++$++ 人寿保险的现金转出价值是否应纳税？...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  0 ++$++ disability-insurance ++$++ 法律要求残疾保险吗？ ...\n",
       "1  1 ++$++ life-insurance ++$++ 债权人可以在死后人寿保险吗？ ++...\n",
       "2  2 ++$++ renters-insurance ++$++ 旅行者保险有租赁保险吗？ +...\n",
       "3  3 ++$++ auto-insurance ++$++ 我可以开一辆没有保险的新车吗？ +...\n",
       "4  4 ++$++ life-insurance ++$++ 人寿保险的现金转出价值是否应纳税？..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看前5个\n",
    "load_insurance_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12889"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数据总长度\n",
    "len(load_insurance_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取中文列\n",
    "content = load_insurance_data[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数: content:string\n",
    "# 方法: 按照'++$++'切分字符串\n",
    "def split_centent(content): \n",
    "    lines = []\n",
    "    for line in load_insurance_data[0].tolist():\n",
    "        lines.append([delete_special_symbol(item.strip())[0] for item in line.split('++$++')])\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['法律要求残疾保险吗', '债权人可以在死后人寿保险吗', '旅行者保险有租赁保险吗', ..., '有人可以为别人购买人寿保险吗',\n",
       "       '我需要多少家庭保险', 'Geico有健康保险吗'], dtype='<U44')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centent_list = split_centent(content)\n",
    "insurance_text = np.array(centent_list)[:,2]\n",
    "insurance_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/dp/9d1tlc2926x2djj4c05f3325ps5m7_/T/jieba.cache\n",
      "Loading model cost 0.819 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('保险', 4904),\n",
       " ('的', 3089),\n",
       " ('人寿保险', 2865),\n",
       " ('什么', 2621),\n",
       " ('是', 2314),\n",
       " ('吗', 2296),\n",
       " ('我', 1881),\n",
       " ('是否', 1802),\n",
       " ('可以', 1615),\n",
       " ('健康', 1487)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 统计单个词列表\n",
    "insurance_token_one = get_all_token(insurance_text.tolist())\n",
    "word_count_one = Counter(insurance_token_one)\n",
    "word_count_one.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('健康保险', 1323),\n",
       " ('什么是', 1148),\n",
       " ('保险是否', 957),\n",
       " ('我的', 687),\n",
       " ('残疾保险', 646),\n",
       " ('房主保险', 599),\n",
       " ('是否覆盖', 496),\n",
       " ('保险吗', 473),\n",
       " ('我可以', 463),\n",
       " ('年金', 455)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 统计两个词列表\n",
    "insurance_token_two = get_all_token_two(insurance_token_one)\n",
    "word_count_two = Counter(insurance_token_two)\n",
    "word_count_two.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数: contents:list(string)\n",
    "# 方法: 测试选择的句子出现的概率，并排序\n",
    "def test_centence_prob(centences):\n",
    "    sentence_prob_map = {}\n",
    "    for sentence in centences:\n",
    "        prob_sentence = get_prob_sentence(sentence, token=insurance_token_two)\n",
    "        sentence_prob_map[sentence] = prob_sentence\n",
    "#         print(sentence, prob_sentence)\n",
    "    return sorted(sentence_prob_map.items(), key=lambda item: item[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "汽车保险是否预付 1.8408977571333628e-10\n",
      "退伍军人能否获得人寿保险 2.4977243221217084e-15\n",
      "法律要求残疾保险吗 3.3889045522186455e-20\n",
      "如何报告年金收入 3.3889045522186455e-20\n",
      "AAA家庭保险涵盖什么 3.3889045522186455e-20\n",
      "社会保险残疾保险是什么 3.3889045522186455e-20\n",
      "医疗保险B部分盖什么 3.3889045522186455e-20\n",
      "分配风险汽车保险如何工作 3.3889045522186455e-20\n",
      "全覆盖汽车保险盖修理 3.3889045522186455e-20\n",
      "旅行者保险有租赁保险吗 4.598055102531302e-25\n",
      "什么是简单的退休计划 4.598055102531302e-25\n",
      "健康保险是否覆盖管道逆转 4.598055102531302e-25\n",
      "如果您已经诊断为乳腺癌 4.598055102531302e-25\n",
      "债权人可以在死后人寿保险吗 6.238626789318347e-30\n",
      "人生在伊斯兰教中是否可以接受 6.238626789318347e-30\n",
      "我可以开一辆没有保险的新车吗 1.1484674064048257e-39\n",
      "人寿保险的现金转出价值是否应纳税 1.1484674064048257e-39\n",
      "我的房主保险是否包括失去的结婚戒指 1.1484674064048257e-39\n",
      "我的男朋友可以加我的汽车保险吗 1.1484674064048257e-39\n",
      "我是否需要提交私人财产车祸索赔的警察报告 2.1142110725914136e-49\n"
     ]
    }
   ],
   "source": [
    "# 选择前20条测试\n",
    "select_centences = insurance_text.tolist()[:20]\n",
    "sentence_prob_map = test_centence_prob(select_centences)\n",
    "for k,v in sentence_prob_map: print(k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2、豆瓣评论数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3020: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# 导入数据\n",
    "load_db_data = pd.read_csv('/Users/yuanjin/PycharmProjects/nlp_data_set/datasource/movie_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京意淫到了脑残的地步，看了恶心想吐</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>中二得很</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                        link name  \\\n",
       "0  1  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "1  2  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "2  3  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "3  4  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "4  5  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "\n",
       "                                             comment star  \n",
       "0                                 吴京意淫到了脑残的地步，看了恶心想吐    1  \n",
       "1  首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...    2  \n",
       "2  吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...    2  \n",
       "3                      凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。    4  \n",
       "4                                               中二得很    1  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看前5个\n",
    "load_db_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['吴京意淫到了脑残的地步，看了恶心想吐',\n",
       " '首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮番上场，视物理逻辑于不顾，不得不说有钱真好，随意胡闹',\n",
       " '吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋律，为了煽情而煽情，让人觉得他是个大做作、大谎言家。（7.29更新）片子整体不如湄公河行动，1.整体不够流畅，编剧有毒，台词尴尬；2.刻意做作的主旋律煽情显得如此不合时宜而又多余。',\n",
       " '凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。',\n",
       " '中二得很']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments = load_db_data['comment'].tolist()\n",
    "comments[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261497"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_comments(comments): \n",
    "    return [''.join(delete_special_symbol(line)) for line in comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261497"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_comment_list = split_comments(comments)\n",
    "len(split_comment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('的', 328262),\n",
       " ('了', 102420),\n",
       " ('是', 73106),\n",
       " ('我', 50338),\n",
       " ('都', 36255),\n",
       " ('很', 34712),\n",
       " ('看', 34022),\n",
       " ('电影', 33675),\n",
       " ('也', 32065),\n",
       " ('和', 31290)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 统计单个词\n",
    "db_token_one = get_all_token(split_comment_list)\n",
    "word_count_1 = Counter(db_token_one)\n",
    "word_count_1.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('的电影', 8640),\n",
       " ('看的', 7106),\n",
       " ('都是', 6335),\n",
       " ('让人', 5284),\n",
       " ('的故事', 4709),\n",
       " ('看了', 4585),\n",
       " ('也是', 4408),\n",
       " ('的时候', 4398),\n",
       " ('的人', 4356),\n",
       " ('的是', 4348)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 统计两个词\n",
    "db_token_two = get_all_token_two(db_token_one)\n",
    "word_count_2 = Counter(db_token_two)\n",
    "word_count_2.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数: contents:list(string)\n",
    "# 方法: 测试选择的句子出现的概率，并排序\n",
    "def test_centence_prob_db(centences):\n",
    "    sentence_prob_map = {}\n",
    "    for sentence in centences:\n",
    "        prob_sentence = get_prob_sentence(sentence, token=db_token_two)\n",
    "        sentence_prob_map[sentence] = prob_sentence\n",
    "#         print(sentence, prob_sentence)\n",
    "    return sorted(sentence_prob_map.items(), key=lambda item: item[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中二得很 1.3567968739400024e-05\n",
      "凭良心说好看到不像战狼1的续集完虐湄公河行动 2.1142110725914136e-49\n",
      "脑子是个好东西希望编剧们都能有 2.1142110725914136e-49\n",
      "吴京意淫到了脑残的地步看了恶心想吐 2.8685549741413693e-54\n",
      "犯我中华者虽远必诛吴京比这句话还要意淫一百倍 3.892046421640054e-59\n",
      "首映礼看的太恐怖了这个电影不讲道理的完全就是吴京在实现他这个小粉红的英雄梦各种装备轮番上场视物理逻辑于不顾不得不说有钱真好随意胡闹 7.999242615820134e-181\n",
      "15100吴京的冷峰在这部里即像成龙又像杰森斯坦森但体制外的同类型电影主角总是代表个人无能的政府需要求助于这些英雄才能解决难题体现的是个人的价值所以主旋律照抄这种模式实际上是有问题的我们以前嘲笑个人英雄主义却没想到捆绑爱国主义的全能战士更加难以下咽 3.02656074676e-312\n",
      "吴京的炒作水平不输冯小刚但小刚至少不会用主旋律来炒作吴京让人看了不舒服为了主旋律而主旋律为了煽情而煽情让人觉得他是个大做作大谎言家729更新片子整体不如湄公河行动1整体不够流畅编剧有毒台词尴尬2刻意做作的主旋律煽情显得如此不合时宜而又多余 4.106428e-317\n",
      "三星半实打实的7分第一集在爱国主旋律内部做着各种置换与较劲但第二集才真正显露吴京的野心他终于抛弃李忠志了新增外来班底让硬件实力有机会和国际接轨开篇水下长镜头和诸如铁丝网拦截RPG弹头的细节设计都让国产动作片重新封顶在理念上它甚至做到绣春刀2最想做到的那部分 0.0\n",
      "开篇长镜头惊险大气引人入胜结合了水平不俗的快剪下实打实的真刀真枪让人不禁热血沸腾特别弹簧床架挡炸弹空手接碎玻璃弹匣割喉等帅得飞起就算前半段铺垫节奏散漫主角光环开太大等也不怕作为一个中国人两个小时弥漫着中国强大得不可侵犯的氛围还是让那颗民族自豪心砰砰砰跳个不停 0.0\n"
     ]
    }
   ],
   "source": [
    "# 选择前10条测试\n",
    "select_centences = split_comment_list[:10]\n",
    "sentence_prob_map = test_centence_prob(select_centences)\n",
    "for k,v in sentence_prob_map: print(k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、获得最优质的的语言\n",
    "    \n",
    "    当我们能够生成随机的语言并且能判断之后，我们就可以生成更加合理的语言了。\n",
    "    请定义 generate_best 函数，该函数输入一个语法 + 语言模型，能够生成n个句子，并能选择一个最合理的句子:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数: token_one表示整个词料库(list(string)), grammer表示生成句子语法模板(string), n(int)表示生成n个句子\n",
    "# 方法: 字动生成n个句子，并计算其出现的概率，再排序\n",
    "def generate_best(token_one, grammer, n):\n",
    "    # 生成2个词的词料库\n",
    "    token_two = [all_token[i]+all_token[i+1] for i in range(len(all_token)-1)]\n",
    "    # 词频统计\n",
    "    word_count = Counter(token_two)\n",
    "    # 生成语法模板\n",
    "    grammer_map = create_grammer(grammer)\n",
    "    # 生成n个句子,并计算句子的概率\n",
    "    sentence_prob_map = {}\n",
    "    for _ in range(n):\n",
    "        centence = create_centence(grammer_map)\n",
    "        prob_sentence = get_prob_sentence(sentence, token_two)\n",
    "        sentence_prob_map[centence] = prob_sentence\n",
    "    # 根据概率排序\n",
    "    prob_map = sorted(sentence_prob_map.items(), key=lambda item: item[1], reverse=True)\n",
    "    # 返回概率最大的语句\n",
    "    return prob_map[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q: 这个模型有什么问题？ 你准备如何提升？\n",
    "    1、一些通用的词（如：的，了，是等）占比极高，需要去除这些通用词的影响\n",
    "    2、grammer_2只考虑了一个词与下一个词的关系，可能这样还是不够的，应该多考虑前后几个词之间的联合概率\n",
    "    3、个人觉得在把单个词的词料库变为相连两次的词料库时，变换有问题，因为没有去掉标点符号前后两个词的组合情况"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
