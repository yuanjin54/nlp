{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import tensorflow as tf\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.test.utils import datapath\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../../nlp_data_set/datasource/movie_comments.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京意淫到了脑残的地步，看了恶心想吐</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>中二得很</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                        link name  \\\n",
       "0  1  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "1  2  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "2  3  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "3  4  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "4  5  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "\n",
       "                                             comment star  \n",
       "0                                 吴京意淫到了脑残的地步，看了恶心想吐    1  \n",
       "1  首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...    2  \n",
       "2  吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...    2  \n",
       "3                      凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。    4  \n",
       "4                                               中二得很    1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据分析和预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(261497, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2760"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(comments_data['name'])) # 电影数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 261497 entries, 0 to 261496\n",
      "Data columns (total 5 columns):\n",
      "id         261497 non-null object\n",
      "link       261497 non-null object\n",
      "name       261497 non-null object\n",
      "comment    261495 non-null object\n",
      "star       261497 non-null object\n",
      "dtypes: object(5)\n",
      "memory usage: 10.0+ MB\n"
     ]
    }
   ],
   "source": [
    "comments_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_data.dropna(subset=['comment', 'star', 'name'], inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 261495 entries, 0 to 261496\n",
      "Data columns (total 5 columns):\n",
      "id         261495 non-null object\n",
      "link       261495 non-null object\n",
      "name       261495 non-null object\n",
      "comment    261495 non-null object\n",
      "star       261495 non-null object\n",
      "dtypes: object(5)\n",
      "memory usage: 12.0+ MB\n"
     ]
    }
   ],
   "source": [
    "comments_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, '1', 2, '2', 3, '3', 4, '4', '5', 5, 'star'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(comments_data['star'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_index = comments_data[comments_data['star'] == 'star'].index\n",
    "comments_data = comments_data.drop(error_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 261494 entries, 0 to 261496\n",
      "Data columns (total 5 columns):\n",
      "id         261494 non-null object\n",
      "link       261494 non-null object\n",
      "name       261494 non-null object\n",
      "comment    261494 non-null object\n",
      "star       261494 non-null object\n",
      "dtypes: object(5)\n",
      "memory usage: 12.0+ MB\n"
     ]
    }
   ],
   "source": [
    "comments_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = comments_data['comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_words_path = 'cut_words_result.txt'\n",
    "save_movie_model = 'movie_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_chinese(word):\n",
    "    for ch in list(word):\n",
    "        if ch < u'\\u4e00' or ch > u'\\u9fff':\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def cut_word(comments):\n",
    "    with open(save_words_path, 'w', encoding='utf8') as f:\n",
    "        for comment in tqdm(comments):\n",
    "            words = jieba.cut(comment)\n",
    "            com_words = []\n",
    "            for word in words:\n",
    "                if not is_chinese(word): continue\n",
    "                com_words.append(word)\n",
    "            f.write(' '.join(com_words)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/261494 [00:00<?, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/dp/9d1tlc2926x2djj4c05f3325ps5m7_/T/jieba.cache\n",
      "Loading model cost 0.998 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "100%|██████████| 261494/261494 [00:56<00:00, 4666.82it/s]\n"
     ]
    }
   ],
   "source": [
    "cut_word(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = LineSentence(save_words_path)\n",
    "model = Word2Vec(sentences, size=300, min_count=100, window=7)\n",
    "model.save(save_movie_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(save_movie_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('狗屎', 0.6862924098968506),\n",
       " ('脑残', 0.6309102773666382),\n",
       " ('浪费时间', 0.6212705373764038),\n",
       " ('奇葩', 0.6080566048622131),\n",
       " ('圈钱', 0.5859946012496948),\n",
       " ('屎', 0.5807947516441345),\n",
       " ('骗钱', 0.5799087882041931),\n",
       " ('尼玛', 0.5606838464736938),\n",
       " ('抄', 0.5552768707275391),\n",
       " ('棒子', 0.5498322248458862)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('垃圾')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('好笑', 0.6597771644592285),\n",
       " ('难看', 0.6569291353225708),\n",
       " ('耐看', 0.611913800239563),\n",
       " ('过瘾', 0.5967444181442261),\n",
       " ('好玩', 0.58929443359375),\n",
       " ('有意思', 0.5863815546035767),\n",
       " ('漂亮', 0.5550641417503357),\n",
       " ('带劲', 0.5428954362869263),\n",
       " ('带感', 0.5409732460975647),\n",
       " ('吓人', 0.5346270203590393)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('好看')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv['垃圾'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 向量化（TF-IDF）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contents(texts):\n",
    "    contents = []\n",
    "    for text in tqdm(texts):\n",
    "        sentence = ''.join(re.findall(r'\\w+', text))\n",
    "        contents.append(' '.join(jieba.cut(sentence)))\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 261494/261494 [00:42<00:00, 6109.74it/s]\n"
     ]
    }
   ],
   "source": [
    "_comments = get_contents(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=100)\n",
    "vectors = vectorizer.fit_transform(_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((261494, 100), (261494,))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, comments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "_y = comments_data['star'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [int(i) for i in _y if str(i).isdigit()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = np.column_stack((X, np.array(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        1.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        2.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        2.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.48153594, 0.        ,\n",
       "        2.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        3.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        3.        ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集拆分(训练、测试、验证)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(all_list, shuffle=False, test_ratio=0.2, vaild_ratio=0.2):\n",
    "    num = len(all_list)\n",
    "    offset1 = int(num * (1 - (test_ratio + vaild_ratio)))\n",
    "    if shuffle:\n",
    "        random.shuffle(all_list)  # 列表随机排序\n",
    "    last_list = all_list[offset1:]\n",
    "    offset2 = offset1 + int(len(last_list) * test_ratio/(test_ratio + vaild_ratio))\n",
    "    return all_list[:offset1], all_list[offset1:offset2], all_list[offset2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, vaild_data = split_data(all_data, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((156896, 101), (52299, 101), (52299, 101))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, test_data.shape, vaild_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_data[:,:-1].astype(np.float32)\n",
    "train_labels = train_data[:,-1:].astype(np.float32).reshape(-1)\n",
    "test_dataset = test_data[:,:-1].astype(np.float32)\n",
    "test_labels = test_data[:,-1:].astype(np.float32).reshape(-1)\n",
    "valid_dataset = vaild_data[:,:-1].astype(np.float32)\n",
    "valid_labels = vaild_data[:,-1:].astype(np.float32).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((156896, 100), (52299, 100), (52299, 100))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.shape, test_dataset.shape, valid_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((156896,), (52299,), (52299,))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape, test_labels.shape, valid_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (156896, 100) (156896, 5)\n",
      "Validation set (52299, 100) (52299, 5)\n",
      "Test set (52299, 100) (52299, 5)\n"
     ]
    }
   ],
   "source": [
    "num_labels = 5\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.astype(np.float32)\n",
    "    # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:, None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "num_steps = 1000\n",
    "hidden_nodes = 300\n",
    "lables_n = 5\n",
    "n_length = train_dataset.shape[0]\n",
    "n_features = train_dataset.shape[1]\n",
    "lr = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 14.607672\n",
      "Minibatch accuracy: 14.2%\n",
      "Validation accuracy: 17.1%\n",
      "Minibatch loss at step 100: 2.170747\n",
      "Minibatch accuracy: 17.6%\n",
      "Validation accuracy: 21.6%\n",
      "Minibatch loss at step 200: 2.086308\n",
      "Minibatch accuracy: 22.2%\n",
      "Validation accuracy: 22.1%\n",
      "Minibatch loss at step 300: 1.863888\n",
      "Minibatch accuracy: 21.0%\n",
      "Validation accuracy: 22.1%\n",
      "Minibatch loss at step 400: 1.721129\n",
      "Minibatch accuracy: 20.6%\n",
      "Validation accuracy: 22.2%\n",
      "Minibatch loss at step 500: 1.781792\n",
      "Minibatch accuracy: 24.4%\n",
      "Validation accuracy: 22.7%\n",
      "Minibatch loss at step 600: 1.881704\n",
      "Minibatch accuracy: 21.4%\n",
      "Validation accuracy: 23.1%\n",
      "Minibatch loss at step 700: 1.916193\n",
      "Minibatch accuracy: 23.8%\n",
      "Validation accuracy: 23.0%\n",
      "Minibatch loss at step 800: 1.865577\n",
      "Minibatch accuracy: 24.6%\n",
      "Validation accuracy: 23.0%\n",
      "Minibatch loss at step 900: 1.772483\n",
      "Minibatch accuracy: 27.0%\n",
      "Validation accuracy: 23.1%\n",
      "Test accuracy: 23.7%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # datasets\n",
    "    train_X = tf.placeholder(tf.float32, shape=(batch_size, n_features))\n",
    "    train_y = tf.placeholder(tf.float32, shape=(batch_size, lables_n))\n",
    "    valid_X = tf.constant(valid_dataset)\n",
    "    test_X = tf.constant(test_dataset)\n",
    "\n",
    "    # --------Create the first layer (1024 hidden nodes)--------\n",
    "    W1 = tf.Variable(tf.truncated_normal([n_features, hidden_nodes]))\n",
    "    b1 = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(train_X, W1), b1))\n",
    "\n",
    "    # --------Create second layer (10 hidden nodes)--------\n",
    "    W2 = tf.Variable(tf.truncated_normal([hidden_nodes, lables_n]))\n",
    "    b2 = tf.Variable(tf.zeros([lables_n]))\n",
    "    logits = tf.add(tf.matmul(layer_1, W2), b2)\n",
    "\n",
    "    # Minimize error using cross entropy\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=train_y, logits=logits))\n",
    "\n",
    "    # Gradient Descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    pre_train_y = tf.nn.relu(tf.add(tf.matmul(train_X, W1), b1))\n",
    "    train_prediction = tf.nn.softmax(tf.add(tf.matmul(pre_train_y, W2), b2))\n",
    "\n",
    "    valid_y = tf.nn.relu(tf.add(tf.matmul(valid_X, W1), b1))\n",
    "    valid_prediction = tf.nn.softmax(tf.add(tf.matmul(valid_y, W2), b2))\n",
    "\n",
    "    test_y = tf.nn.relu(tf.add(tf.matmul(test_X, W1), b1))\n",
    "    test_prediction = tf.nn.softmax(tf.add(tf.matmul(test_y, W2), b2))\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    losses = []\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "        feed_dict = {train_X : batch_data, train_y : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], \n",
    "                                        feed_dict=feed_dict)\n",
    "        if (step % 100 == 0): \n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), \n",
    "                                                           valid_labels))\n",
    "            losses.append(l)\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "    plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
